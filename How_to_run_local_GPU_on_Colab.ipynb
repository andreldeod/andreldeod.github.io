{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOEKJbaVpvCLaNIAnEFxEju",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andreldeod/andreldeod.github.io/blob/main/How_to_run_local_GPU_on_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to set up Colab to run on local GPU\n",
        "\n",
        "##I used the [Dive into Deep Learning](https://d2l.ai/chapter_installation/index.html), [Google instructions](https://research.google.com/colaboratory/local-runtimes.html), and [CUDA instructions](https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index.html).\n",
        "\n",
        "##Steps:\n",
        "\n",
        "1.   Install CUDA toolkit to use with your NVIDIA graphics card: [download installer here](https://developer.nvidia.com/cuda-downloads?target_os=Windows&target_arch=x86_64&target_version=11&target_type=exe_local)\n",
        "\n",
        "    - Make sure that your NVIDIA drivers are up to date\n",
        "    - Verify that your graphics card works with CUDA:\n",
        "       - For Windows go to Device Manager\n",
        "       - Check name of graphics card under Display Adapters\n",
        "       - See if graphics card is on this [list](https://developer.nvidia.com/cuda-gpus).\n",
        "\n",
        "3. Install [Docker Desktop](https://docs.docker.com/get-docker/).\n",
        "4. Open WSL, it should have come with the docker install.\n",
        "5. Run this code in WSL:\n",
        "```\n",
        "docker run --gpus=all -p 127.0.0.1:9000:8080 us-docker.pkg.dev/colab-images/public/runtime\n",
        "```\n",
        "5. Once it runs, copy the http address at the end of the output.\n",
        "6. Open the Colab notebook you want to use with your local GPU.\n",
        "7. Click the down arrow next to 'Connect' on the top right of the Colab page.\n",
        "8. Select 'Connect to a local runtime'.\n",
        "9. Paste the http address you copied from WSL and click 'Connect'.\n",
        "\n",
        "That's it! You should be running on your GPU now. You will not be able to connect to your google drive while running Colab locally, so you will have to download any files you need from the drive.\n",
        "\n",
        "If you want to test if colab is running on the GPU you can run this code. If your gpu is working, it will print 'coda', otherwise 'cpu':\n",
        "\n",
        "```\n",
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HPnTmUEnhdKJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7BZXKjeEhaEa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f280560f-ce16-40a5-f0e0-afc34e04fba4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    }
  ]
}